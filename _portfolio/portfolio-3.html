---
title: "Robotics"
excerpt: "<img src='/images/129_bot.jpg'>"
collection: portfolio
---


<html>

<details>

    <summary><b> Classic Manipulators, Kinematics, Dynamics - CS/EE/ME 133a </b>
    </summary>
    <p>
        <p> </p>
        <p>
        The ME/EE/CS 133a course is the first in the core robotics sequence at Caltech focusing on kinematics and dynamics of robotic systems. 
        For our final project, we evaluated the performance of the Boston Dynamics Atlas robot cutting with the right hand and reaching 
        to grab an item on a neighboring shelf with the left. We approached this problem in two ways: First, we defined the cutting motion of the 
        right arm as the primary task and the left hand as the secondary task. Second, we defined both the right 
        and left hand tasks as primary tasks. We described our results including the tradeoffs between both approaches 
        in our final report and video.
        </p>
        <p>Links to project: 
           <a href="/files/ME133a_FinalProject.pdf" target="_blank">Final Report</a>,
           <a href="https://github.com/avi-patel1/ME133a" target="_blank">Github</a>

        </p>  
        Below is a demonstration of our final project.
        </p>
        <video width="700" height="400" controls>
            <source src="/images/133a_final.mp4" type="video/mp4"></video>
</details>

<details>
    <summary><b> Experimental Robotics - CS/EE/ME 129 </b>
    </summary>
    <p>
        <p> </p>
        I built a mobile, line and wall following robot as part of the CS/EE/ME 129 course at Caltech.
        The hardware includes three IR sensors for line following, end of road
        and intersection detection behaviors, a magnetometer board to measure the orientation 
        of the robot and three ultrasonic sensors to detect blockages in front and at the sides 
        of the robot.
        
        <img src="/images/129_Bot_rename.png" width="500" height="500" class="align-center"
        alt="Diagram">
        
        <p> </p>
        For the robot's software, I wrote the autonomous navigation algorithms, UI and mapping. 
        At every intersection, there are eight possible headings for the robot to explore. 
        Each heading (streets) is denoted by the following status colored on the map shown in the video below:
        
        <ul>
            <li><span style="color:black;"> Black</span>: existence of the street is unknown</li>
            <li> <span style="color:blue;"> Blue</span>: street exists but is unexplored</li>
            <li><span style="color:green;"> Green</span>: street is fully explored (connect to another intersection)</li>
            <li><span style="color:red;"> Red</span>: street has a dead end</li>
        </ul>
        I implemented Dijkstra's algorithm to find the optimal path to a goal node set by the user. 
        If the node is not on the current map, I implemented Dijkstra to the nearest nodes with unexplored headings
        while also exploring the map in the direction of the goal.
        Blockages are denoted by a red X on the map in the video below. The robot is able to replan around static blockages
        along with perform a panic u-turn on moving blockages. 

        Below is a demonstration of the robot where we run directed explore to some goal node. 
    </p>
    <video width="700" height="500" controls>
        <source src="/images/129Demo.mp4" type="video/mp4"></video>
</details>
    

</html>